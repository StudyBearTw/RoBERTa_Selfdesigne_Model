# ğŸ“° ä¸­æ–‡çœŸå‡æ–°èè¾¨è­˜æ¨¡å‹ï¼ˆRoBERTa è‡ªè¡Œè¨­è¨ˆç‰ˆï¼‰

æœ¬å°ˆæ¡ˆç‚ºä¸€å€‹åŸºæ–¼ RoBERTa æ¶æ§‹ï¼Œä¸¦è‡ªè¡Œå¯¦ä½œçš„ä¸­æ–‡çœŸå‡æ–°èè¾¨è­˜æ¨¡å‹ã€‚æ•´é«”ç³»çµ±æ¶µè“‹å¾è³‡æ–™é è™•ç†ã€åµŒå…¥å±¤è¨­è¨ˆã€ç·¨ç¢¼å™¨å¯¦ä½œåˆ°åˆ†é¡ä»»å‹™çš„å®Œæ•´æµç¨‹ï¼Œé©åˆä½œç‚ºè‡ªç„¶èªè¨€è™•ç†ï¼ˆNLPï¼‰å¯¦å‹™æ‡‰ç”¨èˆ‡ç ”ç©¶çš„åƒè€ƒã€‚

> ğŸ“˜ æœ¬å°ˆæ¡ˆç‚ºå¤§å­¸å°ˆé¡Œæˆæœä¹‹ä¸€ï¼Œç”± [StudyBearTw](https://github.com/StudyBearTw) è£½ä½œèˆ‡ç¶­è­·ã€‚

---

## ğŸ” å°ˆæ¡ˆç°¡ä»‹

- **ä»»å‹™é¡å‹**ï¼šä¸­æ–‡æ–°èäºŒåˆ†é¡ï¼ˆçœŸ / å‡ï¼‰
- **æ¨¡å‹æ¶æ§‹**ï¼šæ¨¡ä»¿ RoBERTa è¨­è¨ˆï¼Œä¸¦ç”±é›¶é–‹å§‹å¯¦ä½œï¼ˆä¸ä¾è³´ Hugging Face ç­‰ç¾æˆæ¨¡å‹ï¼‰
- **Tokenizer**ï¼šä½¿ç”¨å“ˆçˆ¾æ¿±å·¥æ¥­å¤§å­¸çš„hfl/chinese-bert-wwm-ext Tokenizer
- **è¼¸å…¥æ ¼å¼**ï¼šæ–°èæ¨™é¡Œèˆ‡å…§æ–‡çµ„åˆç‚ºå–®ä¸€è¼¸å…¥
- **è¨“ç·´è³‡æ–™**ï¼šçœŸå¯¦ä¸–ç•Œçš„ä¸­æ–‡æ–°èè³‡æ–™é›†ï¼ˆè©³è¦‹ `data/`ï¼‰

---

## ğŸ§  æ¨¡å‹æ¶æ§‹

æœ¬æ¨¡å‹ä¸»è¦åˆ†ç‚ºä»¥ä¸‹å¹¾å€‹æ¨¡çµ„ï¼ˆçš†ç‚º `.py` æª”æ¡ˆå½¢å¼ï¼‰ï¼š

- `embeddings/`: å¯¦ä½œ RoBERTa åµŒå…¥å±¤ï¼ˆtokenã€positionã€segment embeddingï¼‰
- `encoder/`: å¤šå±¤ Transformer ç·¨ç¢¼å™¨ï¼Œæ¯å±¤å…·å‚™è‡ªæ³¨æ„åŠ›èˆ‡å‰é¥‹ç¥ç¶“ç¶²è·¯
- `model/`: çµåˆåµŒå…¥å±¤èˆ‡ç·¨ç¢¼å™¨çš„ä¸»æ¨¡å‹é¡åˆ¥
- `pretrain.py`: æ¨¡å‹é è¨“ç·´ä¸»ç¨‹å¼
- `fine_tune.py`: æ¨¡å‹å¾®èª¿è¨“ç·´ä¸»ç¨‹å¼
- `test_model.py`: æ¸¬è©¦èˆ‡é©—è­‰æ¨¡çµ„æ•ˆèƒ½

---

## ğŸ› ï¸ å®‰è£èˆ‡åŸ·è¡Œæ–¹å¼

### 1. ç’°å¢ƒå®‰è£

è«‹å…ˆå®‰è£ Python å¥—ä»¶ï¼š

```bash
pip install -r requirements.txt
````

> âœ… å»ºè­°ä½¿ç”¨ Python 3.8+ èˆ‡ PyTorch 1.10+

### 2. è³‡æ–™æº–å‚™

å°‡è¨“ç·´èˆ‡æ¸¬è©¦è³‡æ–™æ”¾å…¥ `data/` è³‡æ–™å¤¾ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š

```
data/
â”œâ”€â”€ train.csv
â”œâ”€â”€ test.csv
```

è³‡æ–™æ ¼å¼åƒè€ƒï¼š

```csv
{
  "title": "æ–°èæ¨™é¡Œ",
  "label": 1
}
```

### 3. æ¨¡å‹è¨“ç·´
#### pre-train
```bash
python train_mlm.py
```
#### fine-tune
```bash
python run_fine_tune.py
```

### 4. æ¨¡å‹è©•ä¼°

```bash
python test_model.py
```

---

## ğŸ“ˆ æˆæ•ˆèˆ‡è©•ä¼°

* è©•ä¼°æŒ‡æ¨™ï¼šAccuracyï¼ˆæº–ç¢ºç‡ï¼‰, Precisionï¼ˆç²¾ç¢ºç‡ï¼‰, Recallï¼ˆå¬å›ç‡ï¼‰, F1 scoreï¼ˆF1 åˆ†æ•¸ï¼‰, Confusion matrixï¼ˆæ··æ·†çŸ©é™£ï¼‰
* æ¸¬è©¦çµæœé¡¯ç¤ºæœ¬æ¨¡å‹åœ¨é©—è­‰é›†ä¸Šæœ‰è‰¯å¥½çš„åˆ†é¡èƒ½åŠ›ï¼ˆè©³è¦‹ `results/`ï¼‰

---

## ğŸ“ å°ˆæ¡ˆæ¶æ§‹

```

ROBERTA\_SELFDESIGNE\_MODEL/
â”œâ”€â”€ Dataset/
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ Fine\_Tune\_model/
â”‚   â”œâ”€â”€ Fine-Tune\_result.md
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ Pre-Train\_result.md
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ RoBERTa\_Custom/
â”‚   â”œâ”€â”€ **pycache**/
â”‚   â”œâ”€â”€ **init**.py
â”‚   â”œâ”€â”€ attention.py
â”‚   â”œâ”€â”€ embeddings.py
â”‚   â”œâ”€â”€ encoder.py
â”‚   â”œâ”€â”€ fine\_tune.py
â”‚   â”œâ”€â”€ mlm\_loss.py
â”‚   â”œâ”€â”€ model.py
â”‚   â”œâ”€â”€ pretrain.py
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ .gitattributes
â”œâ”€â”€ checkDoc.py
â”œâ”€â”€ combine\_dataset.py
â”œâ”€â”€ Dataset\_Check.py
â”œâ”€â”€ dataset\_file\_to\_csv.py
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ run\_fine\_tune.py
â”œâ”€â”€ test\_model.py
â”œâ”€â”€ test.py
â””â”€â”€ train\_mlm.py
```

---

## ğŸ“š åƒè€ƒè³‡æº

* [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)
* [HuggingFace Transformers (ä½œç‚ºæ¶æ§‹åƒè€ƒ)](https://github.com/huggingface/transformers)
* [Google BERT Paper](https://arxiv.org/abs/1810.04805)

---

## ğŸ“¬ è¯çµ¡æ–¹å¼

å¦‚æœ‰ä»»ä½•å•é¡Œæˆ–å»ºè­°ï¼Œæ­¡è¿è¯çµ¡æˆ‘ï¼š

* GitHub: [StudyBearTw](https://github.com/StudyBearTw)
* Email: `studyspiderpig@gmail.com`

---

## ğŸ“„ æˆæ¬Šæ¢æ¬¾

è«‹éµå¾ªç›¸é—œæ¨¡å‹çš„æˆæ¬Šæ¢æ¬¾ï¼š
- Chinese BERT WWM: Apache 2.0 License

---

## ğŸ“œ License

æœ¬å°ˆæ¡ˆæ¡ç”¨ MIT æˆæ¬Šæ¢æ¬¾ï¼Œè©³è¦‹ `LICENSE` æª”æ¡ˆã€‚

